%%%%%%%%%%%%%%%%%%          gtlatex.tem       %%%%%%%%%%%%%%%%%%
%
%  Template for articles written in LaTeX for publication in
%  G&T, G&TM and A&GT.  This template must be used with latex2e.  
%  If you use BiBTeX then you can collect the bibliography style 
%  file  gtart.bst  from the same directory as this file.  Full
%  instructions for using gtpart.cls are given in gtpartdoc.pdf.  
%
%
\documentclass{gtpart}     
%
%\usepackage{pinlabel}  %%% the recommended graphics+labelling package
\usepackage{graphicx}  %%% the recommended graphics package
\usepackage{mathtools}
\usepackage{bbm}
\usepackage[mathscr]{euscript}
\usepackage{enumerate}


%%% Start of metadata
%

\title{A Survey of Dimensionality Reduction Techniques}

%  First author
%
\author{Rachel Levanger}
\givenname{Rachel}
\surname{Levanger}
\address{}
\email{rachel@math.rutgers.edu}
\urladdr{}

%  Second author (uncomment if necessary)
%
%\author{}
%\givenname{}
%\surname{}
%\address{}
%\email{}
%\urladdr{}
%
%  (Add a similar block for other authors)
%
%   Title and author both have running head options:
%
%   \title[Running head title]{Main title}
%   \author[Running head author]{Author}
%
% give a separate \keyword and \subject line for each keyword/phrase or 
% subject class eg \keyword{framed link} \subject{primary}{msc2000}{57M25}

\keyword{}
\subject{primary}{msc2000}{}

%
%  fill in the reference and password if your article is stored at the
%  arXiv eg \arxivreference{math.GT/0512347}  \arxivpassword{5spud}

\arxivreference{}
\arxivpassword{}

%
%  Leave the following items blank
%
\volumenumber{}
\issuenumber{}
\publicationyear{}
\papernumber{}
\startpage{}
\endpage{}
\doi{}
\MR{}
\Zbl{}
\received{}
\revised{}
\accepted{}
\published{}
\publishedonline{}
\proposed{}
\seconded{}
\corresponding{}
\editor{}
\version{}

%%% End of metadata
%
%%% Start of user-defined macros %%%
%
%   Theorem-type environments.  There are two predefined styles :
%
%   \theoremstyle{plain} : for theorems, corollaries etc with heading 
%   bold and left justified, optional note bracketed in roman type
%   and statement in slanted type.  This is the default style.
%
%   \theoremstyle{definition} : (alias remark)  for definitions, remarks 
%   etc with heading bold and left justified, optional note as before but
%   with statement in roman type.
%   
%   Some sample  \newtheorem's  (delete these unless you need
%   them and insert your own):
%
\newtheorem{thm}{Theorem}[section]    % Standard theorem environment
\newtheorem{lem}[thm]{Lemma}          % Lemma environment with numbering 
%                                     % consecutive to theorems
\newtheorem{prop}[thm]{Proposition}          % Proposition environment with numbering 
%                                     % consecutive to theorems
\newtheorem{cor}[thm]{Corollary}          % Corollary environment with numbering 
%                                     % consecutive to theorems
\newtheorem*{zlem}{Zorn's Lemma}      % A special unnumbered lemma.
%
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}    % Definition environment with 
%                                     % numbering consecutive to theorems
\newtheorem{ex}[thm]{Example}    % Example environment with 
%                                     % numbering consecutive to theorems
\newtheorem*{rem}{Remark}             % Unnumbered environment for remarks.
%
%   Type your macros (\newcommand's etc) below.
%

% Common spaces
\newcommand{\mbb}[1]{\mathbb{#1}}
\newcommand{\mcal}[1]{\mathcal{#1}}
\newcommand{\mbm}[1]{\mathbbm{#1}}
\newcommand{\mscr}[1]{\mathscr{#1}}

% Arrows
\newcommand{\larr}{\leftarrow}
\newcommand{\rarr}{\rightarrow}
\newcommand{\lrarr}{\leftrightarrow}
\newcommand{\Larr}{\Leftarrow}
\newcommand{\Rarr}{\Rightarrow}
\newcommand{\LRarr}{\Leftrightarrow}

% Greeks
\newcommand{\lbd}{\lambda}
\newcommand{\eps}{\varepsilon}

% Functions
\newcommand{\X}[1]{\chi_{#1}}

% Formatting
\newcommand{\ul}[1]{\underline{#1}}

% Misc
\newcommand{\ip}[2]{\langle {#1},{#2}\rangle}


%%%% MACROS SPECIFIC TO THIS PAPER


%%% End of user-defined macros %%%

\begin{document}

\begin{abstract}    % type your abstract below

This note set gives a short summary of a collection of commonly-used linear and non-linear dimensionality reduction techniques.

\end{abstract}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%   Start of main body of article

\pagebreak 

\section{Linear Dimensionality Reduction (LDR) Techniques}

\subsection{Principal Component Analysis (PCA)} 

PCA is a tool used to describe a data set in terms of its extrinsic (linear) directions of highest variance, and does so in a way that the data is uncorrelated with respect to these directions. Consider a set $S$ of $m$ points in $\R^n$. Then $S$ can be represented as an $m \times n$ matrix $X$. Call $\mu_X = (\mu_1, ..., \mu_n)$ the center of the data set, or the average of each of the columns of $X$. The general sequence of steps to perform a PCA on $X$ is as follows:
\begin{enumerate}[(1)]
\item Traditionally, compute the covariance matrix $\Sigma$ associated to the data set $X$, where $$\Sigma_{ij} = \text{cov}(X_{*i}, X_{*j}) =(1/m) \sum_{k=1}^m (X_{ki} - \mu_i)(X_{kj} - \mu_j).$$ Notice that this is just the dot product of the mean-centered column vectors of the data matrix divided by the number of points in $S$. It is also possible to use $\Sigma = X^TX$ or the correlation matrix, as these are all essentially functions of the dot product, they are real and symmetric, and so diagonalization yields an orthogonal set of eigenvectors. Using $\Sigma = X^TX$ is essentially the computation of the Singular Value Decomposition (SVD) of $X$ as it relates to finding the directions of highest variance.
\item Since $\Sigma$ is real and symmetric, we can diagonalize $\Sigma$ via the matrices $\Phi$ and $\Lambda$, where $\Phi$ is orthogonal and $\Lambda$ is diagonal. The matrix $\Phi$ has columns that form the orthonormal basis of eigenvectors of $\Sigma$, $\Lambda$ gives the corresponding eigenvalues, and the equation $\Sigma = \Phi \Lambda \Phi^T$ holds.
\item The magnitude of the eigenvalues in $\Lambda$ provide insight into the number of dimensions along which the data is organized. Furthermore, since the eigenvectors are orthogonal, the covariance along any two transformed basis vectors are zero (their dot product is zero).
\end{enumerate}

sources:  \\
https://stat.duke.edu/~sayan/Sta613/2015/lec/IDAPILecture15.pdf \\
http://infolab.stanford.edu/~ullman/mmds/ch11.pdf \\

\subsection{Multidimensional scaling (MDS)}




\section{Non-linear Dimensionality Reduction (NLDR) Techniques}

\subsection{Kernel PCA}

\subsection{Isomap}

\subsection{Locally-linear Embedding (LLE)}

This technique works best when the points in your dataset are assumed to have densely sampled a portion of a manifold that can be `flattened out' in some sense. For instance, the swiss roll is a two-dimensional manifold with boundary that can be flattened out into a portion of a two-dimensional plane, while a torus cannot be flattened out as such and would not be a good candidate for this method.

Locally-linear embedding (LLE) rests on the assumption that points that are nearest to a target point lie on the same portion of the manifold as the target point, i.e. the sampling density is much smaller than the weak feature size of the underlying manifold. Let $X = \{x_1, ..., x_N\}$ be a set of points in $\R^D$, $K \in \mathbb{N}$, and $d < D$ be the desired embedding dimension. 

{\bf Algorithm:}
\begin{enumerate}[(1)]
\item {\it Find the $K$ nearest neighbors of each point.} Compute the distance matrix of the data set $X$. Let $N_K:X \rightarrow \mathscr{P}(X)$ be the function that assigns to each $x_i$ its $K$ nearest neighbors, $N_K(x_i)$, which can be gleaned from the distance matrix.
\item {\it Solve for the reconstruction weights.} 
	\begin{enumerate}[(a)]
	\item For each $x_i \in X$, create a matrix $Z_i$ with a column for each element in $N_K(x_i)$. Subtract $x_i$ from each column in $Z$ (center at $x_i$). 
	\item Set $C_i = Z_i^T Z_i$ ($K$ times the local covariance), and solve for a vector $w_i$ such that $C_i w_i = [1]$, where $[1]$ is a column vector of all ones. 
	\item Populate the $i^{th}$ row of the weight matrix $W$ as follows. If $x_j \notin N_K(x_i)$, set $W_{ij} = 0$. If $x_j \in N_K(x_i)$ then set $W_{ij} = w_j/(w \cdot [1])$, where $w_j$ is the entry in $w$ corresponding to the neighbor $x_j$.
	\end{enumerate}
\item {\it Compute the embedding coordinates.} Set $M = (I - W)^T(I - W)$. Find the $d + 1$ eigenvectors corresponding to the smallest eigenvalues of $M$. Set the $q^{th}$ row of $Y$ to be the eigenvector corresponding to the $(q+1)^{th}$-smallest eigenvalue. (Thus, the eigenvector corresponding to the smallest eigenvalue of $M$ is not used in $Y$.
\end{enumerate}

sources:\\
https://www.cs.nyu.edu/~roweis/lle/algorithm.html

\subsection{Hessian Locally-linear Embedding (Hessian LLE)}

\subsection{Laplacian Eigenmaps}

\subsection{Diffusion Maps}


\end{document}

